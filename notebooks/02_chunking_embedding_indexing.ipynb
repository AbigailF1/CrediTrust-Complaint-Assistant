{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc677d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed dataset: (82164, 19)\n",
      "Splitting narratives into chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82164/82164 [00:35<00:00, 2318.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total rows: 82164, Total chunks created: 422805\n",
      "Generating embeddings in batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f8a94dea6b4099b278466eb81c2d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings shape: (422805, 384)\n",
      "✅ FAISS index contains 422805 vectors\n",
      "FAISS index and metadata saved to vector_store/\n"
     ]
    }
   ],
   "source": [
    "# 02_chunking_embedding_indexing.py\n",
    "# Chunking, Embeddings, and Vector Store Indexing for CrediTrust Complaints\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Imports\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load preprocessed dataset\n",
    "# -----------------------------\n",
    "processed_file = \"../data/processed/filtered_complaints.csv\"  # ✅ path fixed for src folder\n",
    "df = pd.read_csv(processed_file)\n",
    "print(f\"Loaded preprocessed dataset: {df.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Text chunking\n",
    "# -----------------------------\n",
    "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].fillna('').astype(str)\n",
    "\n",
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,      # reasonable size for semantic chunks\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "metadata = []\n",
    "\n",
    "print(\"Splitting narratives into chunks...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    narrative = row['Consumer complaint narrative'].strip()\n",
    "    if not narrative:\n",
    "        continue\n",
    "\n",
    "    split_texts = text_splitter.split_text(narrative)\n",
    "    chunks.extend(split_texts)\n",
    "\n",
    "    for _ in split_texts:\n",
    "        metadata.append({\n",
    "            'complaint_id': row['Complaint ID'],\n",
    "            'product': row['Product']\n",
    "        })\n",
    "\n",
    "print(f\"✅ Total rows: {len(df)}, Total chunks created: {len(chunks)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Embeddings\n",
    "# -----------------------------\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "print(\"Generating embeddings in batches...\")\n",
    "embeddings = embedder.encode(chunks, show_progress_bar=True, batch_size=64)\n",
    "print(f\"✅ Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. FAISS vector store\n",
    "# -----------------------------\n",
    "embedding_dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(embeddings)\n",
    "print(f\"✅ FAISS index contains {index.ntotal} vectors\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save FAISS index and metadata\n",
    "# -----------------------------\n",
    "os.makedirs(\"vector_store\", exist_ok=True)\n",
    "\n",
    "faiss.write_index(index, \"vector_store/complaints_faiss.index\")\n",
    "with open(\"vector_store/metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"FAISS index and metadata saved to vector_store/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
